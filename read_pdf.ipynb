{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ollama import chat\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import sys\n",
    "from typing import Dict, List, Pattern, Tuple, Optional\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from openpyxl.styles import Font, Border, Side, Alignment, PatternFill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load The PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 documents into split_documents.json ðŸš€\n"
     ]
    }
   ],
   "source": [
    "# Load PDF\n",
    "# pdf_path = \"DocComplaints.pdf\"\n",
    "pdf_path = \"DocComplaints_Original.pdf\"\n",
    "name_file = pdf_path.split(\".\")[0]\n",
    "doc = PdfReader(pdf_path)\n",
    "\n",
    "# Extract text\n",
    "full_text = \"\"\n",
    "for page in doc.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        full_text += \"\\n\" + text\n",
    "\n",
    "# Improved regex to split at exact section numbers (e.g., '1.', '2.', ..., '15.') only at line start\n",
    "documents = re.split(r\"(?m)(?=^\\d{1,2}\\.\\s)\", full_text)\n",
    "\n",
    "# Clean up\n",
    "documents = [doc.strip() for doc in documents if doc.strip()]\n",
    "\n",
    "# Create JSON-style output\n",
    "json_output = []\n",
    "for idx, content in enumerate(documents):\n",
    "    json_output.append({\n",
    "        \"source\": pdf_path,\n",
    "        \"content\": content\n",
    "    })\n",
    "\n",
    "document_json = f'{name_file}_split_documents.json'\n",
    "\n",
    "# Save as JSON\n",
    "with open(document_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(json_output)} documents into split_documents.json ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANONYMIZE\n",
    "\n",
    "class GermanTextAnonymizer:\n",
    "    \"\"\"Anonymizer for German text that uses spaCy NER and regex patterns.\"\"\"\n",
    "\n",
    "    def __init__(self, nlp: Language):\n",
    "        \"\"\"\n",
    "        Initialize the anonymizer with a spaCy language model.\n",
    "\n",
    "        Args:\n",
    "            nlp: Loaded spaCy language model\n",
    "        \"\"\"\n",
    "        self.nlp = nlp\n",
    "        self.entity_types: Dict[str, str] = {}\n",
    "        self.regex_patterns: List[Tuple[str, Pattern, str]] = []\n",
    "\n",
    "    def add_entity_type(self, entity_type: str, replacement: str) -> None:\n",
    "        \"\"\"\n",
    "        Add an entity type to be anonymized.\n",
    "\n",
    "        Args:\n",
    "            entity_type: The spaCy entity label to anonymize\n",
    "            replacement: Text to replace the entity with\n",
    "        \"\"\"\n",
    "        self.entity_types[entity_type] = replacement\n",
    "\n",
    "    def add_regex_pattern(self, name: str, pattern: str, replacement: str) -> None:\n",
    "        \"\"\"\n",
    "        Add a regex pattern to be anonymized.\n",
    "\n",
    "        Args:\n",
    "            name: Name of the pattern for reference\n",
    "            pattern: Regular expression pattern to match\n",
    "            replacement: Text to replace matches with\n",
    "        \"\"\"\n",
    "        self.regex_patterns.append(\n",
    "            (name, re.compile(pattern, re.IGNORECASE), replacement)\n",
    "        )\n",
    "\n",
    "    def anonymize(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Anonymize the text using NER and regex patterns.\n",
    "\n",
    "        Args:\n",
    "            text: The text to anonymize\n",
    "\n",
    "        Returns:\n",
    "            Anonymized text with PII replaced\n",
    "        \"\"\"\n",
    "        # Process text with spaCy\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        # Collect all replacements to be made\n",
    "        replacements = []\n",
    "\n",
    "        # Add entity replacements from spaCy NER\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in self.entity_types:\n",
    "                replacements.append(\n",
    "                    (ent.start_char, ent.end_char, self.entity_types[ent.label_])\n",
    "                )\n",
    "\n",
    "        # Add regex pattern replacements\n",
    "        for name, pattern, replacement in self.regex_patterns:\n",
    "            for match in pattern.finditer(text):\n",
    "                replacements.append((match.start(), match.end(), replacement))\n",
    "\n",
    "        # Sort replacements in reverse order by start position\n",
    "        # This way, replacing from end to beginning doesn't affect other positions\n",
    "        replacements.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Apply replacements from end to beginning\n",
    "        result = text\n",
    "        for start, end, replacement in replacements:\n",
    "            result = result[:start] + replacement + result[end:]\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def load_spacy_model(model_name: str = \"de_core_news_md\") -> Optional[Language]:\n",
    "    \"\"\"\n",
    "    Load the specified spaCy model.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name of the spaCy model to load\n",
    "\n",
    "    Returns:\n",
    "        Loaded spaCy model or None if loading failed\n",
    "    \"\"\"\n",
    "    print(f\"Loading {model_name} model...\")\n",
    "    try:\n",
    "        return spacy.load(model_name)\n",
    "    except OSError:\n",
    "        print(f\"Error: {model_name} not found.\")\n",
    "        print(f\"Please install it with: python -m spacy download {model_name}\")\n",
    "        return None\n",
    "\n",
    "def configure_anonymizer(anonymizer: GermanTextAnonymizer) -> None:\n",
    "    \"\"\"\n",
    "    Configure the anonymizer with entity types and regex patterns.\n",
    "\n",
    "    Args:\n",
    "        anonymizer: The anonymizer to configure\n",
    "    \"\"\"\n",
    "    # Configure entity types to anonymize\n",
    "    entity_mappings = {\n",
    "        \"PER\": \"[PERSON]\",\n",
    "        \"ORG\": \"[ORGANISATION]\",\n",
    "        \"LOC\": \"[ORT]\",\n",
    "        \"GPE\": \"[ORT]\",\n",
    "        \"MONEY\": \"[FINANZIELL]\",\n",
    "        \"DATE\": \"[DATUM]\",\n",
    "        \"CARDINAL\": \"[NUMMER]\"\n",
    "    }\n",
    "\n",
    "    for entity_type, replacement in entity_mappings.items():\n",
    "        anonymizer.add_entity_type(entity_type, replacement)\n",
    "\n",
    "    # Add regex patterns for additional PII\n",
    "    regex_patterns = [\n",
    "        (\"PLZ\", r'\\b\\d{5}\\b', \"[POSTLEITZAHL]\"),\n",
    "        (\n",
    "            \"ADDRESS\",\n",
    "            r'(\\b\\w+(?:straÃŸe|str\\.|weg|platz|allee|gasse)\\s+\\d+,?\\s+\\d{5}\\s+\\w+\\b)',\n",
    "            \"[ADRESSE]\"\n",
    "        ),\n",
    "        (\n",
    "            \"AGE\",\n",
    "            r'\\b\\d{1,3}[-\\s]?(?:jÃ¤hrige[rn]?|Jahre[n]? alt|Jahre)\\b',\n",
    "            \"[ALTER]\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for name, pattern, replacement in regex_patterns:\n",
    "        anonymizer.add_regex_pattern(name, pattern, replacement)\n",
    "\n",
    "\n",
    "### LLAMA \n",
    "\n",
    "# Base function\n",
    "def chat_with_ollama(prompt: str, model: str = \"llama3\") -> str:\n",
    "    response = chat(\n",
    "        model=model,\n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': prompt\n",
    "        }]\n",
    "    )\n",
    "    return response.message.content\n",
    "\n",
    "# Summarize function\n",
    "def summarize_text(text: str, model: str = \"llama3\") -> str:\n",
    "    prompt = f\"\"\"\n",
    "        Here is a statement from a public participation process:\n",
    "        \n",
    "        {text}\n",
    "        \n",
    "        Please provide:\n",
    "        1. A concise summary (max 150 words) that captures all unique concerns\n",
    "        2. A list of the main themes/topics mentioned\n",
    "        Note: The output text should be in german.\n",
    "        \n",
    "        Format your response as:\n",
    "        SUMMARY: [Your summary here]\n",
    "        TOPICS: [comma-separated list of themes]\n",
    "        \"\"\"\n",
    "    return chat_with_ollama(prompt, model=model)\n",
    "\n",
    "\n",
    "# Key points function\n",
    "def classify_text(text: str, model: str = \"llama3\") -> str:\n",
    "    prompt = f\"\"\"You are a classification assistant.\n",
    "\n",
    "Read the following text and assign it to one of the following categories based on its content:\n",
    "\n",
    "CATEGORIES:\n",
    "- Siedlung\n",
    "- Natur und Artenschutz\n",
    "- Wasser\n",
    "- Wald\n",
    "- Bodenschutz\n",
    "- Infrastruktur\n",
    "- FlÃ¤chenqualitÃ¤t\n",
    "- Landschaft\n",
    "- Andere\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "Format your response as:\n",
    "CATEGORY: [category]\"\"\"\n",
    "    \n",
    "    return chat_with_ollama(prompt, model=model)\n",
    "\n",
    "# Key points summurize function\n",
    "def summarize_keypoints(text: str, model: str = \"llama3\") -> str:\n",
    "    prompt = f\"\"\"\n",
    "You are a summurization assistant.\n",
    "\n",
    "The following text is a dictionry with the main clusters and the found keypoints for each cluster. \n",
    "Your task is to summarize the keypoints for each cluster, the output should be the most 5 repeated keypoints for each cluster.\n",
    "Note: \n",
    "1- the keypoints could be repeated in different words, but the meaning should be the same.\n",
    "2- return only the output without any additional text.\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "Format your response as a json file:\n",
    "\"CLUSTER\": [list of keypoints],...\n",
    "\"\"\"\n",
    "    \n",
    "    return chat_with_ollama(prompt, model=model)\n",
    "\n",
    "\n",
    "def read_json_list(filepath: str) -> List[Dict]:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"JSON root element must be a list\")\n",
    "        return data\n",
    "    \n",
    "def write_json_list(data: List[Dict], filepath: str) -> None:\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    match = re.search(r'\\{\\s*\".*?\\}\\s*$', text, re.DOTALL)\n",
    "    if match:\n",
    "        json_str = match.group(0)\n",
    "        return json.loads(json_str)\n",
    "    else:\n",
    "        raise ValueError(\"No valid JSON object found in the text.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading de_core_news_md model...\n",
      "Done 1/5\n",
      "\n",
      "Done 2/5\n",
      "\n",
      "Done 3/5\n",
      "\n",
      "Done 4/5\n",
      "\n",
      "Done 5/5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON file\n",
    "document_list = read_json_list(document_json)\n",
    "\n",
    "# Load German spaCy model\n",
    "nlp = load_spacy_model()\n",
    "if not nlp:\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize and configure anonymizer\n",
    "anonymizer = GermanTextAnonymizer(nlp)\n",
    "configure_anonymizer(anonymizer)\n",
    "\n",
    "file_statements = f\"{name_file}_statements.json\"\n",
    "\n",
    "for n,i in enumerate(document_list):\n",
    "    anonymized_text = anonymizer.anonymize(i['content'])\n",
    "    document_list[n]['text_anonym'] = anonymized_text\n",
    "\n",
    "for n,i in enumerate(document_list):\n",
    "    # print(f'Text {n}\\n --- {i['content']}\\n')\n",
    "    text_summary = summarize_text(i['text_anonym'])\n",
    "    # key_points = list_key_points(i['content'])\n",
    "    \n",
    "    summary_part, themes_part = text_summary.split(\"TOPICS:\")\n",
    "    categories = classify_text(i['text_anonym'])\n",
    "    _,categories_new = categories.split(sep=':')\n",
    "    categories = categories_new.split(',')\n",
    "    categories = [i.strip() for i in categories]\n",
    "\n",
    "    summary = summary_part.replace(\"SUMMARY:\", \"\").strip()\n",
    "    topics = [t.strip() for t in themes_part.split(\",\")]\n",
    "\n",
    "    document_list[n]['summary'] = summary\n",
    "    document_list[n]['key_points'] = topics\n",
    "    document_list[n]['cluster'] = categories\n",
    "    \n",
    "    print(f'Done {n+1}/{len(document_list)}\\n')\n",
    "    # break\n",
    "\n",
    "\n",
    "write_json_list(document_list,file_statements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data\n",
    "with open(file_statements, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize dictionary to collect keywords per cluster\n",
    "cluster_keywords = defaultdict(list)\n",
    "\n",
    "for doc in data:\n",
    "    clusters = doc.get(\"cluster\", [])\n",
    "    keywords = doc.get(\"key_points\", [])\n",
    "    for cluster in clusters:\n",
    "        cluster_keywords[cluster].extend(keywords)\n",
    "\n",
    "# Print or use the result\n",
    "keypoints = json.dumps(cluster_keywords, indent=2, ensure_ascii=False)\n",
    "\n",
    "summarized_keypoints = summarize_keypoints(keypoints)\n",
    "result = extract_json_from_text(summarized_keypoints)\n",
    "\n",
    "keypoints_name_file = f\"{name_file}_keypoints.json\"\n",
    "\n",
    "write_json_list(result, keypoints_name_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown table created and saved as 'cluster_summary_table.md'\n",
      "Excel file created and saved as 'cluster_summary.xlsx'\n",
      "Excel file updated with key topics for each cluster\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file\n",
    "with open(file_statements, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create a dictionary to store the unique clusters and their content\n",
    "clusters = defaultdict(list)\n",
    "\n",
    "# Process each document in the data\n",
    "for doc in data:\n",
    "    # Handle cluster as either a string or a list\n",
    "    if 'cluster' in doc:\n",
    "        if isinstance(doc['cluster'], list):\n",
    "            cluster_names = doc['cluster']\n",
    "        else:\n",
    "            cluster_names = [doc['cluster']]\n",
    "    else:\n",
    "        cluster_names = ['Uncategorized']\n",
    "    \n",
    "    # Combine key points and summary for each document\n",
    "    doc_content = \"\"\n",
    "    \n",
    "    # Add document ID if available\n",
    "    if 'document_id' in doc:\n",
    "        doc_content += f\"**Doc ID:** {doc['document_id']}\\n\\n\"\n",
    "    elif 'Document ID' in doc:\n",
    "        doc_content += f\"**Doc ID:** {doc['Document ID']}\\n\\n\"\n",
    "    \n",
    "    # Add summary if available\n",
    "    if 'summary' in doc and doc['summary']:\n",
    "        doc_content += f\"**Summary:** {doc['summary']}\\n\\n\"\n",
    "    elif 'Summary' in doc and doc['Summary']:\n",
    "        doc_content += f\"**Summary:** {doc['Summary']}\\n\\n\"\n",
    "    \n",
    "    # Add key points if available\n",
    "    if 'key_points' in doc and doc['key_points']:\n",
    "        if isinstance(doc['key_points'], list):\n",
    "            doc_content += \"**Key Points:**\\n\"\n",
    "            for point in doc['key_points']:\n",
    "                doc_content += f\"- {point}\\n\"\n",
    "        else:\n",
    "            doc_content += f\"**Key Points:** {doc['key_points']}\\n\\n\"\n",
    "    \n",
    "    # Add the content to each appropriate cluster\n",
    "    if doc_content:\n",
    "        for cluster_name in cluster_names:\n",
    "            clusters[cluster_name].append(doc_content)\n",
    "\n",
    "# Find the maximum number of items in any cluster for table sizing\n",
    "max_items = max(len(items) for items in clusters.values())\n",
    "\n",
    "# Create a DataFrame for the markdown table\n",
    "df = pd.DataFrame(index=range(max_items), columns=sorted(clusters.keys()))\n",
    "\n",
    "# Fill the DataFrame with the content\n",
    "for cluster in clusters:\n",
    "    for i, content in enumerate(clusters[cluster]):\n",
    "        df.loc[i, cluster] = content\n",
    "\n",
    "# Generate the markdown table\n",
    "markdown_table = df.fillna(\"\").to_markdown()\n",
    "\n",
    "# Write to a file\n",
    "with open('cluster_summary_table.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(markdown_table)\n",
    "\n",
    "print(\"Markdown table created and saved as 'cluster_summary_table.md'\")\n",
    "# Also create a nicely formatted Excel file\n",
    "\n",
    "# Create a workbook and worksheet\n",
    "workbook = openpyxl.Workbook()\n",
    "worksheet = workbook.active\n",
    "worksheet.title = \"Cluster Summary\"\n",
    "\n",
    "# Add column headers\n",
    "for col_num, column_title in enumerate(sorted(clusters.keys()), 1):\n",
    "    cell = worksheet.cell(row=1, column=col_num)\n",
    "    cell.value = column_title\n",
    "    cell.font = Font(bold=True)\n",
    "    cell.fill = PatternFill(start_color='DDEBF7', end_color='DDEBF7', fill_type='solid')\n",
    "    cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)\n",
    "\n",
    "# Add content rows\n",
    "for cluster_idx, cluster_name in enumerate(sorted(clusters.keys())):\n",
    "    col_num = cluster_idx + 1\n",
    "    for row_num, content in enumerate(clusters[cluster_name], 2):\n",
    "        cell = worksheet.cell(row=row_num, column=col_num)\n",
    "        cell.value = content\n",
    "        cell.alignment = Alignment(wrap_text=True, vertical='top')\n",
    "\n",
    "# Auto-adjust column widths\n",
    "for column in worksheet.columns:\n",
    "    max_length = 0\n",
    "    column_letter = openpyxl.utils.get_column_letter(column[0].column)\n",
    "    for cell in column:\n",
    "        if cell.value:\n",
    "            try:\n",
    "                if len(str(cell.value)) > max_length:\n",
    "                    max_length = min(len(str(cell.value)), 100)  # Cap width at 100 chars\n",
    "            except:\n",
    "                pass\n",
    "    adjusted_width = (max_length + 2)\n",
    "    worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "\n",
    "# Set a border style for all cells with content\n",
    "thin_border = Side(border_style=\"thin\", color=\"000000\")\n",
    "border = Border(left=thin_border, right=thin_border, top=thin_border, bottom=thin_border)\n",
    "\n",
    "for row in worksheet.iter_rows(min_row=1, max_row=worksheet.max_row, min_col=1, max_col=worksheet.max_column):\n",
    "    for cell in row:\n",
    "        if cell.value:\n",
    "            cell.border = border\n",
    "\n",
    "# Save the workbook\n",
    "excel_summary_file = f\"{name_file}_cluster_summary.xlsx\"\n",
    "\n",
    "workbook.save(excel_summary_file)\n",
    "print(\"Excel file created and saved as 'cluster_summary.xlsx'\")\n",
    "\n",
    "\n",
    "import json\n",
    "import openpyxl\n",
    "from openpyxl.styles import Font, Alignment, PatternFill\n",
    "\n",
    "# Save keypoints to JSON file\n",
    "with open(keypoints_name_file, 'r', encoding='utf-8') as f:\n",
    "        keypoints_data = json.load(f)\n",
    "\n",
    "# Open the existing Excel file\n",
    "workbook = openpyxl.load_workbook(excel_summary_file)\n",
    "worksheet = workbook.active\n",
    "\n",
    "# Get column indices for each cluster\n",
    "column_indices = {}\n",
    "for col in range(1, worksheet.max_column + 1):\n",
    "        header = worksheet.cell(row=1, column=col).value\n",
    "        if header in keypoints_data:\n",
    "                column_indices[header] = col\n",
    "\n",
    "# Insert a new row after the header\n",
    "worksheet.insert_rows(2)\n",
    "\n",
    "thin_border = Side(border_style=\"thin\", color=\"000000\")\n",
    "border_bold = Border(left=thin_border, right=thin_border, top=thin_border, bottom=thin_border)\n",
    "\n",
    "# Add keypoints to each column\n",
    "for cluster, col_idx in column_indices.items():\n",
    "        # Join the keypoints with commas\n",
    "        keypoints_text = \", \\n\".join(keypoints_data[cluster])\n",
    "        \n",
    "        # Add to the worksheet\n",
    "        cell = worksheet.cell(row=2, column=col_idx)\n",
    "        cell.value = f\"Key Topics: \\n{keypoints_text}\"\n",
    "        cell.font = Font(italic=True)\n",
    "        cell.alignment = Alignment(wrap_text=True)\n",
    "        cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
    "        cell.border = border_bold\n",
    "\n",
    "# Adjust row height\n",
    "worksheet.row_dimensions[2].height = 120\n",
    "\n",
    "# Save the updated workbook\n",
    "cluster_summary_file = f\"{name_file}_cluster_summary_updated.xlsx\"\n",
    "workbook.save(cluster_summary_file)\n",
    "print(\"Excel file updated with key topics for each cluster\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
